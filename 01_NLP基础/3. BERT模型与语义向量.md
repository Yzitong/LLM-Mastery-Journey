# 3. BERT模型与语义向量

## 前言：

BERT（Bidirectional Encoder Representations from Transformers）是 Google 于 2018 年提出的预训练语言模型，基于 Transformer 架构，是一种端到端的训练方法。BERT 的核心能力是将文本（词、句子）转换为包含**上下文信息**的向量表征。

传统方法（如 Word2Vec）通过 “词共现” 统计生成语义向量，无法捕捉动态语义（如 “苹果” 在 “水果” 和 “公司” 中的不同含义）。而BERT模型可以解决一词多义的问题。

*BERT 与 Word2Vec 有什么关系与区别呢？*BERT 与 Word2Vec**均为语义向量生成工具**，两者均是将文本转换为向量的技术，但**Word2Vec 是静态词向量**，而**BERT 是动态上下文向量**。针对于计算速度要求很高的场景下，如语音客服场景，使用静态的词向量其相应速度更快。



