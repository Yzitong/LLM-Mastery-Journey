# 2. 溢出词表词（OOV）问题

## 1.问题定义

溢出词表词问题在实际场景中很常见。例如，在实时获取的社交评论文本数据中，我们经常会看到包含最新的网络用语且伴随着大量拼写错误的词语，以及各种创造性的表达方式，包括词语的缩写、单词的多种形式（如复数形式、时态变化等）等。这些形态变化丰富的词可能从未在模型的词表中出现过。

如果模型无法处理这些溢出词表词，即模型无法为这些溢出词表词构建合适的词向量，那么它就无法表示这些词的语义信息，由此便会带来 3 个不良的影响。

- 模型在诸多下游任务上的性能变差。例如，对于文本分类任务，当输入文本中包含大量溢出词表词，并且没有有效的词向量表示时，模型对输入文本的语义理解是不完整且不连贯的，这导致模型无法在向量空间中构建正确的分类边界，从而降低了模型分类的准确性。
- 弱化了模型的泛化能力。优秀的自然语言处理模型应该具备良好的泛化能力，即能够处理训练数据之外的新实例。溢出词表词问题意味着模型对于未见过的词汇缺乏处理能力，这就弱化了模型的泛化能力。
- 限制了模型的应用范围。模型在特定领域或语言中的应用效果很大程度上取决于它能否处理该领域或语言中的特有词汇。而溢出词表词问题可能导致模型无法有效地应用于专业领域或多语言环境，这就限制了模型的应用范围。

接下来，我们介绍两种处理溢出词表词问题的思路。

*！面试常考：你了解哪些解决OOV问题的方法？！*

一、

```python
import re
import numpy as np
from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer
import gensim.models.keyedvectors

# 初始化词干提取器
ps = PorterStemmer()
lc = LancasterStemmer()
sb = SnowballStemmer("english")

# 加载词向量模型（这里需根据实际路径调整，确保文件存在）
spell_model = gensim.models.keyedvectors.load_word2vec_format(
    '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'
)
words = spell_model.index2word
w_rank = {}
for i, word in enumerate(words):
    w_rank[word] = i
WORDS = w_rank  # 存储单词排名的全局变量


# 辅助函数
def words(text):
    return re.findall(r'\w+', text.lower())


def P(word):
    # 获取单词在字典中的相反数排名，不在字典中返回 0
    return -WORDS.get(word, 0)


def correction(word):
    # 最有可能的单词拼写纠正
    return max(candidates(word), key=P)


def candidates(word):
    # 为单词生成可能的拼写纠正
    return (known([word]) or known(edits1(word)) or [word])


def known(words):
    # 从 WORDS 字典中获取已知的单词子集
    return set(w for w in words if w in WORDS)


def edits1(word):
    # 所有与 word 相差一个编辑距离的词
    letters = 'abcdefghijklmnopqrstuvwxyz'
    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]
    deletes = [L + R[1:] for L, R in splits if R]
    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]
    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]
    inserts = [L + c + R for L, R in splits for c in letters]
    return set(deletes + transposes + replaces + inserts)


def edits2(word):
    # 所有与 word 相差两个编辑距离的词
    return (e2 for e1 in edits1(word) for e2 in edits1(e1))


def singlify(word):
    # 将单词中的连续重复字母减少到一个
    return ''.join([letter for i, letter in enumerate(word) if i == 0 or letter != word[i - 1]])


def load_glove(word_dict, lemma_dict):
    # 加载 GloVe 词嵌入文件（路径需根据实际调整）
    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'
    embeddings_index = {}

    def get_coefs(o):
        return o.split(" ")[0], np.asarray(o.split(" ")[1:], dtype='float32')

    # 从文件中读取单词和对应的向量，存入字典
    with open(EMBEDDING_FILE) as f:
        for o in f:
            word, arr = get_coefs(o)
            embeddings_index[word] = arr

    embed_size = 300
    nb_words = len(word_dict) + 1
    embedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32)
    # 初始化未知向量，所有元素为 -1
    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1
    print(unknown_vector[:5])

    for key in token(word_dict):
        word = key
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue

        # 尝试不同的形式获取词向量
        word = key.lower()
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue

        word = key.upper()
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue

        word = key.capitalize()
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue

        word = ps.stem(key)
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue

        word = lc.stem(key)
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue

        word = sb.stem(key)
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue

        word = lemma_dict[key]
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue

        # 如果以上方式都无法获取向量，则使用未知向量
        if len(key) > 1:
            word = correction(key)
            embedding_vector = embeddings_index.get(word)
            if embedding_vector is not None:
                embedding_matrix[word_dict[key]] = embedding_vector
                continue

        embedding_matrix[word_dict[key]] = unknown_vector

    return embedding_matrix, nb_words
```

