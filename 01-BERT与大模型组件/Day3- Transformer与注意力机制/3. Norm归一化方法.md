### 3. 归一化的方法

- **LayerNorm（层归一化）**：
  - 计算公式：对于输入 x，计算均值 μ 和方差 σ^2 沿特征维度。
  - 归一化：\( \hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} \)。
  - 缩放和偏移：\( y = \gamma \hat{x} + \beta \)，其中 γ 和 β 是可学习的参数。
  - 适用场景：独立于批量大小，适合变长序列。

- **BatchNorm（批归一化）**：
  - 计算公式：沿批量维度计算均值和方差，归一化后缩放和偏移。
  - 适用场景：依赖于批量大小，适合固定大小的批量处理。

- **区别分析**：
  - LayerNorm 在每个样本内归一化，BatchNorm 在批量内归一化。
  - Transformer 选择 LayerNorm，因为序列长度可变，批归一化不适用。

- **为什么 Transformer 选择 LayerNorm**：
  - Transformer 处理变长序列，LayerNorm 不依赖批量大小，更稳定。
  - 序列长度可变，LayerNorm 更稳定，适合深层网络。