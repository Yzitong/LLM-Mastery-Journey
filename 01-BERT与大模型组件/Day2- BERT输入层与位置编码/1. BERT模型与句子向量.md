# 1. BERT模型与句子向量

## 前言：

BERT（Bidirectional Encoder Representations from Transformers）是 Google 于 2018 年提出的预训练语言模型，基于 Transformer 架构，是一种端到端的训练方法。BERT 的核心能力是将文本（词、句子）转换为包含**上下文信息**的向量表征。

传统方法（如 Word2Vec）通过 “词共现” 统计生成语义向量，无法捕捉动态语义（如 “苹果” 在 “水果” 和 “公司” 中的不同含义）。而BERT模型可以解决一词多义的问题。

*BERT 与 Word2Vec 有什么关系与区别呢？*BERT 与 Word2Vec**均为语义向量生成工具**，两者均是将文本转换为向量的技术，但**Word2Vec 是静态词向量**，而**BERT 是动态上下文向量**。针对于计算速度要求很高的场景下，如语音客服场景，使用静态的词向量其相应速度更快。



## 为什么 BERT 更适合表征句子向量？

1. **从词向量到句子向量的天然优势**：
   - Word2Vec 仅能生成词向量，若要表征句子，需通过平均池化、加权求和等方法组合词向量，但这种方式忽略了词语间的语义结构（如主谓宾关系）。
   - BERT 的 Transformer 架构能直接建模句子中词语的上下文依赖，其输出的词向量已包含句子级语义信息，只需通过简单池化（如取 CLS token 向量）即可得到句子向量。
2. **语义表征能力的跃升**：
   - BERT 预训练时接触了海量语料（如 BooksCorpus、Wikipedia），学习到了更丰富的语言知识（如语法、常识），句子向量能捕捉更深层次的语义关联。
   - **实验对比**：在文本相似度任务中，BERT 句子向量对 “近义词替换”“主动句 - 被动句转换” 等语义变化的表征精度远高于 Word2Vec。
3. **任务适应性与迁移能力**：
   - BERT 通过微调可直接应用于各类 NLP 任务（如分类、问答），其句子向量能自适应任务需求，而 Word2Vec 需配合额外特征工程。

Word2Vec 解决了 “词语数字化” 的基础问题，但 BERT 通过上下文建模实现了从 “符号表征” 到 “语义理解” 的跨越。当前常用 BERT 表征句子向量，本质是因为自然语言处理任务对 “语境感知” 和 “深层语义表征” 的需求不断提升。尽管 BERT 仍有优化空间，但其动态表征能力已成为现代 NLP 任务的核心基础。
