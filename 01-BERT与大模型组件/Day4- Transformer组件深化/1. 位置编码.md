# 1. 位置编码

在自然语言处理（NLP）中，**位置编码**是Transformer模型处理序列顺序信息的核心技术。由于Transformer架构的自注意力机制（Self-Attention）是**排列不变**的，即不感知输入序列中元素的顺序，因此需要通过位置编码为模型提供序列的顺序信息。位置编码分为两大类：**绝对位置编码**和**相对位置编码**，它们在建模位置信息的方式上存在根本差异。

## 一、**绝对位置编码：给每个位置 “贴唯一标签”**

### 1. 定义

**绝对位置编码**为序列中的每个位置赋予一个**唯一的固定编码**，并将其嵌入输入的词向量中。例如，位置$$3$$始终对应一个固定的向量，不随上下文变化。分为 **可学习位置编码**（如 BERT）和 **三角位置编码**（如 GPT 早期、Transformer 论文）。

### 2. 实现方式

#### 2.1 可学习位置编码

- **描述**：通过训练学习一个位置编码矩阵，每个位置对应一个可更新的向量。BERT等模型广泛采用此方法。

- 公式：

  $$ \text{Input}_i = \text{TokenEmbedding}_i + \text{PosEmbedding}_i + \text{SegEmbedding}_i $$

  - 参数说明：
    - $$ \text{Input}_i $$：第$$i$$个位置的最终输入向量。
    - $$ \text{TokenEmbedding}_i $$：词嵌入向量。
    - $$ \text{PosEmbedding}*i $$：位置嵌入向量，来自矩阵$$ \mathbf{PE} \in \mathbb{R}^{\text{maxLen} \times d*{\text{model}}} $$。
    - $$ \text{SegEmbedding}_i $$：段嵌入向量（如BERT中区分句子对）。

- **优点**：灵活性高，能适应不同任务中的位置模式。

- **缺点**：受限于训练时的最大长度$$ \text{maxLen} $$，推理时超长输入可能导致性能下降。

#### 2.2 三角位置编码

- **描述**：利用正弦和余弦函数生成固定的位置编码，最早由Transformer论文提出。

- 公式：

  $$ PE_{(\text{pos}, 2i)} = \sin\left( \dfrac{\text{pos}}{10000^{2i/d}} \right) $$

  $$ PE_{(\text{pos}, 2i+1)} = \cos\left( \dfrac{\text{pos}}{10000^{2i/d}} \right) $$

  - 参数说明：
    - $$ \text{pos} $$：位置索引。
    - $$ d $$：嵌入维度（需为偶数）。
    - $$ i $$：维度索引，$$ 0 \leq i < d/2 $$。

- **优点**：具有周期性，可外推到训练长度之外的序列。

- **缺点**：固定的公式可能无法捕捉复杂的语义位置关系。

### 3. 典型应用

- **BERT**：采用可学习位置编码，适用于短文本任务。
- **GPT**：早期使用三角位置编码，后期结合可学习编码。

## 三、相对位置编码

### 1. 定义

**相对位置编码**关注序列中元素之间的**相对距离或顺序关系**，而非绝对位置。例如，单词“狗”与“猫”的前后顺序通过编码体现。核心是 **直接建模 token 间的相对距离 / 顺序**，通过修改注意力机制实现。

分为 **基于注意力修改的编码**（T5、XLNet、DeBERTa）、**旋转位置编码（RoPE，如 LLama）**，以及轻量方案 **Alibi**。

### 2. 实现方式

#### 2.1 **基于注意力修改的相对编码**：通过改写注意力计算式，将 “相对距离” 直接融入注意力得分。

#### （1）**T5：可训练偏置 + 分桶策略**

- **描述**：T5在注意力权重中引入相对位置偏差，使用分桶策略将相对距离分组。
- 原理：简化注意力展开式，仅保留 **内容 - 内容交互** 和 **可训练的相对位置偏置** `β_ij`（`β_ij`仅依赖相对距离`i-j`）。
  对`i-j`做 **分桶处理**：近距（如 0~7）每个距离一个桶，远距（如 8~11）共享桶，更远桶更粗（如表格中`i-j=8~11`都映射到`f(i-j)=8`），减少参数。
- 公式： $$ A_{i,j} = \mathbf{x}_i W_Q W_K^T \mathbf{x}*j + \beta*{f(i-j)} $$
  - 参数说明：
    - $$ A_{i,j} $$：第$$i$$个查询和第$$j$$个键的注意力得分。
    - $$ \mathbf{x}_i, \mathbf{x}_j $$：输入向量。
    - $$ W_Q, W_K $$：查询和键的变换矩阵。
    - $$ \beta_{f(i-j)} $$：相对位置偏置，由分桶函数$$ f(\cdot) $$确定。
- **优点**：简化长距离建模，计算开销低。
- **缺点**：分桶可能丢失细粒度信息。

#### （2） **XLNet：分解注意力，学习多维度交互**

- **描述**：XLNet采用相对位置编码并结合双流注意力，通过分解注意力得分显式建模相对位置。

- 原理：将注意力得分分解为四大交互项：

  - 内容 - 内容：`x_i W_Q W_K^T x_j`（词与词的交互）
  - 内容 - 相对位置：`x_i W_Q W_K^R R_{i-j}`（词与相对位置的交互）
  - 位置 - 内容：`u W_K^T x_j`（可训练位置向量与词的交互）
  - 位置 - 相对位置：`v W_K^R R_{i-j}`（可训练位置向量与相对位置的交互）
    模型通过学习这四项，**精细捕捉 “内容 - 位置”“位置 - 内容” 等维度的依赖关系**。相对位置向量`R_{i-j}`类似三角编码生成，更灵活。

- 公式：

   $$ A_{i,j} = \mathbf{x}_i W_Q W_K^T \mathbf{x}_j + \mathbf{x}*i W_Q W_K^R \mathbf{R}*{i-j}^T + \mathbf{u} W_K^T \mathbf{x}*j + \mathbf{v} W_K^R \mathbf{R}*{i-j}^T $$

  - 参数说明：
    - $$ \mathbf{R}_{i-j} $$：相对位置向量。
    - $$ W_K^R $$：相对位置投影矩阵。
    - $$ \mathbf{u}, \mathbf{v} $$：可训练偏置向量。

- **优点**：细致建模内容与位置交互，适合长文本。

- **缺点**：计算复杂度高。

#### （3） **DeBERTa：截断 + 层次化编码**

- **描述**：DeBERTa通过解耦注意力机制分离内容和位置信息，并对相对距离进行截断。
- 原理：
  - 保留 **三项交互**（去掉 “位置 - 位置” 项），通过`δ(t,s)`将相对距离`t-s` **截断到`(-k, k]`区间**（如 k=512，超距按边界处理）。
  - 模型分 **Encoder（前 11 层用相对位置）和 Decoder（后 2 层加绝对位置）**，结合两者优势。
  - softmax 校正系数改为 `√3d`（常规为`√d`），提升稳定性。
- 公式：$$ A_{t,s} = \mathbf{x}_t^T W_Q^T W_K \mathbf{x}_s + \mathbf{x}*t^T W_Q^T W_K \mathbf{P}*{\delta(t,s)} + \mathbf{x}*s^T W_K^T W_Q \mathbf{P}*{\delta(s,t)} $$
  - 参数说明：
    - $$ \delta(t,s) = \text{clip}(t - s, -k, k) $$：截断到$$ [-k, k] $$的相对距离。
    - $$ \mathbf{P}_{\delta} $$：相对距离的编码向量。
- **优点**：精确捕捉位置关系，提升复杂任务表现。
- **缺点**：截断可能丢失长距离信息。

#### 注：T5、XLNet与DeBERTa的相对位置编码对比

以下从**实现方式**、**计算复杂度**、**位置感知能力**和**适用场景**对比T5、XLNet和DeBERTa：

| **维度**         | **T5**                                         | **XLNet**                              | **DeBERTa**                               |
| ---------------- | ---------------------------------------------- | -------------------------------------- | ----------------------------------------- |
| **实现方式**     | 注意力得分加偏置$$ \beta_{f(i-j)} $$，分桶策略 | 注意力得分分解为四项，显式建模相对位置 | 解耦注意力，截断相对距离$$ \delta(t,s) $$ |
| **计算复杂度**   | 低（少量偏置参数）                             | 高（额外矩阵和向量）                   | 中等（截断计算）                          |
| **位置感知能力** | 粗略感知相对距离                               | 细致感知内容与位置交互                 | 精确感知相对位置                          |
| **适用场景**     | 多任务、资源受限                               | 长文本、复杂依赖                       | 高精度任务                                |

- **T5**：通过分桶简化建模，适合计算资源有限的多任务场景。
- **XLNet**：分解注意力得分，细致建模位置交互，适合长文本。
- **DeBERTa**：解耦机制提升精度，适合复杂任务。

#### 2.2 旋转位置编码RoPE （以LLama为例）

- **描述**：通过旋转变换将位置信息嵌入词向量，使模型隐式捕捉相对位置。LLaMA和GPT-J等模型采用此方法。
- **公式**：
  $$
  \mathbf{q}_m' = \begin{pmatrix} \cos(m\theta_i) & -\sin(m\theta_i) \\ \sin(m\theta_i) & \cos(m\theta_i) \end{pmatrix} \begin{pmatrix} q_m^{2i} \\ q_m^{2i+1} \end{pmatrix}
  $$
  - **参数说明**：
    - \($\mathbf{q}_m'$\)：位置 \(m\) 的查询向量经过旋转后的结果。
    - \($\theta_i = \dfrac{1}{10000^{2i/d}}$\)：旋转角度。
    - \($q_m^{2i}, q_m^{2i+1}$\)：查询向量的第 \(2i\) 和 \(2i+1\) 维。
- **推导核心**：
  
  - 查询 \($\mathbf{q}_m'$\) 与键 \($\mathbf{k}_n'$\) 的内积包含相对角度 \($(m - n)\theta_i$\)：
    $$
    \mathbf{q}_m'^T \mathbf{k}_n' = \sum_{i=0}^{d/2-1} \left[ (q_m^{2i}k_n^{2i} + q_m^{2i+1}k_n^{2i+1}) \cos((m-n)\theta_i) + (q_m^{2i+1}k_n^{2i} - q_m^{2i}k_n^{2i+1}) \sin((m-n)\theta_i) \right]
    $$
  - 内积直接感知相对距离 \(m - n\)，实现“绝对编码形式，相对编码效果”。
- **优点**：无需额外参数，与Transformer兼容，长序列处理效果显著。
- **缺点**：数学实现较复杂，需理解旋转矩阵的几何意义。

#### 2.3 补充：Alibi位置编码
- **描述**：为每个注意力头引入线性偏置，随距离线性衰减。
- **公式**：
  $$
  \text{bias}_{h,i,j} = -s_h \cdot |i - j|
  $$
  - **参数说明**：
    - \($\text{bias}_{h,i,j}$\)：第 \(h\) 个注意力头的偏置。
    - \($s_h$\)：可学习的斜率参数。
    - \($|i - j|$\)：位置 \(i\) 和 \(j\) 的相对距离。
- **优点**：简单高效，支持长序列处理。
- **缺点**：线性衰减假设可能不适用于所有任务。

## 四、绝对位置编码与相对位置编码的对比

| **维度**       | **绝对位置编码**                                     | **相对位置编码**                         |
| -------------- | ---------------------------------------------------- | ---------------------------------------- |
| **信息类型**   | 绝对位置（如“第3个词”）                              | 相对距离（如“前2个词”）                  |
| **序列长度**   | 固定长度限制，外推性差（可学习编码）或强（三角编码） | 无固定长度限制，天然支持变长序列         |
| **计算复杂度** | 低（直接相加）                                       | 高（需修改注意力机制）                   |
| **典型场景**   | 短文本任务、固定长度输入                             | 长文本任务、动态序列（如对话、实时生成） |

### 选择建议
- **短文本任务**：优先选择绝对位置编码（如BERT的可学习编码），简单高效。
- **长文本任务**：采用相对位置编码（如RoPE或Alibi），提升长距离依赖建模能力。
- **多任务场景**：考虑T5的简化相对位置编码，统一处理不同任务。

## 五、总结

- **绝对位置编码**为每个位置提供唯一标签，适合短文本和固定长度输入任务。其实现包括可学习位置编码和三角位置编码，分别以灵活性和外推性见长。
- **相对位置编码**聚焦元素间的相对关系，擅长处理长文本和动态序列。典型方法如RoPE和Alibi在长序列任务中表现尤为突出。
- 根据任务需求、序列长度和计算资源选择合适方案至关重要。
- 随着长文本处理需求的增加，相对位置编码及其变种（如RoPE和Alibi）在现代NLP模型中愈发重要。

通过深入理解和合理应用位置编码，可以显著提升Transformer模型在各种NLP任务中的表现。本文提供的公式和分析旨在为读者提供理论与实践的全面参考。

---

**附录：核心公式速览**

### 绝对位置编码
- **可学习位置编码**：
  $$
  \text{Input}_i = \text{TokenEmbedding}_i + \text{PosEmbedding}_i + \text{SegEmbedding}_i
  $$
- **三角位置编码**：
  $$
  PE_{(\text{pos}, 2i)} = \sin\left( \dfrac{\text{pos}}{10000^{2i/d}} \right), \quad PE_{(\text{pos}, 2i+1)} = \cos\left( \dfrac{\text{pos}}{10000^{2i/d}} \right)
  $$

### 相对位置编码
- **T5相对编码**：
  $$
  A_{i,j} = \mathbf{x}_i W_Q W_K^T \mathbf{x}_j + \beta_{f(i-j)}
  $$
- **RoPE**：
  $$
  \mathbf{q}_m' = \begin{pmatrix} \cos(m\theta_i) & -\sin(m\theta_i) \\ \sin(m\theta_i) & \cos(m\theta_i) \end{pmatrix} \begin{pmatrix} q_m^{2i} \\ q_m^{2i+1} \end{pmatrix}
  $$
- **Alibi**：
  $$
  \text{bias}_{h,i,j} = -s_h \cdot |i - j|
  $$