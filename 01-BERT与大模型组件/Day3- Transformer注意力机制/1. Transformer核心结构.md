# 1. Transformer核心结构

#### 引言

Transformer 模型是一种革命性的深度学习架构，于 2017 年由 Vaswani 等人在论文《Attention Is All You Need》中提出。它最初被设计用于机器翻译任务，但由于其强大的能力和灵活性，迅速成为自然语言处理（NLP）领域的基石，并扩展到计算机视觉、语音识别等其他领域。

与传统的循环神经网络（RNN）和卷积神经网络（CNN）相比，Transformer 的主要优势在于其高效的并行化能力和捕捉长距离依赖的能力。这些优势源于其核心的自注意力机制（self-attention），无需依赖 RNN 的循环结构或 CNN 的局部卷积操作。

《Attention is all your need》：[[1706.03762\] Attention Is All You Need](https://arxiv.org/abs/1706.03762)

B站指路：快速了解→[《Attention is all you need》论文解读及Transformer架构详细介绍_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1xoJwzDESD/?spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=aa6afb9d0536d09ecdcb5d2c1fcf4c79)

​		  论文详解→[Transformer论文逐段精读【论文精读】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=aa6afb9d0536d09ecdcb5d2c1fcf4c79)

​		  代码解读→[Transformer代码(源码Pytorch版本)从零解读(Pytorch版本）_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1dR4y1E7aL/?spm_id_from=333.1387.homepage.video_card.click&vd_source=aa6afb9d0536d09ecdcb5d2c1fcf4c79)

#### Transformer 相较于 RNN 和 CNN 的优势
- **RNN 的局限性**：
  - RNN 通过循环结构处理序列数据，但由于其逐步处理的特性，无法实现高效的并行化，尤其在长序列上训练速度较慢。
  - RNN 容易出现梯度消失或梯度爆炸问题，导致在处理长距离依赖时表现不佳。例如，在长句子的翻译中，早期信息可能被遗忘。
  - 文献如 [Transformer 架构](https://zh.wikipedia.org/wiki/Transformer%E6%A8%A1%E5%9E%8B) 指出，RNN 的这些问题限制了其在复杂任务中的表现。

- **CNN 的局限性**：
  - CNN 擅长处理局部特征，但在处理序列数据时，无法直接捕捉全局依赖关系。例如，在句子中，远距离词之间的语义关联可能被忽略。
  - CNN 的卷积核通常假设局部相关性，难以扩展到长序列的全局建模。

- **Transformer 的解决方案**：
  - **自注意力机制**：允许模型直接关注序列中的任意位置，从而捕捉长距离依赖。例如，在翻译任务中，“it”可能与句子的开头词相关，Transformer 可直接建模。
  - **完全并行化**：Transformer 可以同时处理整个序列，显著提高训练和推理速度，尤其适合 GPU 加速。文献如 [理解语言的 Transformer 模型](https://www.tensorflow.org/tutorials/text/transformer?hl=zh-cn) 强调了这一优势。
  - **无需循环或卷积**：通过纯注意力机制，Transformer 避免了 RNN 的循环结构和 CNN 的局部卷积操作，实现了更高的灵活性和效率。

以下表格总结了三者的对比：

| 模型类型    | 并行化能力 | 长距离依赖 | 训练速度 | 适用场景        |
| ----------- | ---------- | ---------- | -------- | --------------- |
| RNN         | 差         | 一般       | 慢       | 短序列任务      |
| CNN         | 一般       | 较弱       | 中等     | 图像、局部特征  |
| Transformer | 优秀       | 优秀       | 快       | NLP、长序列任务 |

#### Transformer 的核心结构
Transformer 包括嵌入（embedding）、编码器（encoder）和解码器（decoder）三部分：  

- 嵌入将词转为向量，并添加位置信息。  
- 编码器处理输入序列，包含多头自注意力机制和前馈网络。  
- 解码器生成输出序列，结合编码器结果和掩码自注意力。

##### 1. 嵌入（Embedding）
- **输入转换**：输入序列（如单词序列）首先被映射到一个固定维度的向量空间中。每个词被转换为一个维度为 \(d_{\text{model}} = 512\) 的嵌入向量。
- **参数**：
  - \(d_{\text{model}} = 512\)：嵌入向量的维度。
  - 嵌入矩阵的大小为 \(|V| \times d_{\text{model}}\)，其中 \(|V|\) 是词汇表的大小。
- **位置编码（Positional Encoding）**：
  - 由于 Transformer 没有循环结构，无法直接捕捉词序信息，因此需要引入位置编码。
  - 位置编码是一个固定函数，不需要学习，其目的是为每个位置的嵌入向量添加位置信息。
  - **计算公式**：
    对于位置 \(pos\) 和维度 \(i\)：
  - \[
    PE(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
    \]
    \[
    PE(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
    \]
    其中 \(d_{\text{model}} = 512\)。
  - **举例说明**：
    - 对于位置 \(pos = 0\) 和维度 \(i = 0\)：
      \[
      PE(0, 0) = \sin\left(\frac{0}{10000^{0/512}}\right) = \sin(0) = 0
      \]
      \[
      PE(0, 1) = \cos\left(\frac{0}{10000^{0/512}}\right) = \cos(0) = 1
      \]
    - 对于位置 \(pos = 1\) 和维度 \(i = 0\)：
      \[
      PE(1, 0) = \sin\left(\frac{1}{10000^{0/512}}\right) = \sin(1) \approx 0.8415
      \]
      \[
      PE(1, 1) = \cos\left(\frac{1}{10000^{0/512}}\right) = \cos(1) \approx 0.5403
      \]
    - 这样，每个位置的嵌入向量都会被添加一个独特的位置编码向量，确保模型能感知序列顺序。

##### 2. 编码器（Encoder）
###### 编码器的详细过程
- **结构**：编码器由 \(N = 6\) 个相同的层组成，每层包含两个子层：
  1. **多头自注意力（Multi-Head Self-Attention）**
  2. **位置-wise 前馈神经网络（Position-wise Feed-Forward Network）**
- **残差连接（Residual Connection）**：
  - 每个子层都有残差连接，即将输入直接添加到子层的输出中。
  - 作用：帮助梯度在深层网络中更好地传播，缓解梯度消失问题。
- **层归一化（Layer Normalization）**：
  - 每个子层之后都应用层归一化。
  - 作用：稳定化训练过程，确保不同层的输入分布一致。

###### 为什么需要 FNN？
- **FNN 的作用**：
  - 前馈神经网络（FNN）是一个两层全连接网络（通常是 ReLU 激活函数），应用于每个位置的向量上。
  - 它引入非线性性，使模型能够学习更复杂的特征表示。
- **举例说明**：
  - 假设输入序列为 "I love cats"，经过自注意力层后，每个词的表示已经包含了序列中其他词的信息。
  - FNN 则进一步处理这些表示，提取更高层次的特征，例如情感或语义信息。

###### 为什么要有残差连接？
- 残差连接通过将输入直接添加到子层的输出中，确保了即使子层学习能力不足，模型也能传递原始信息。
- 这在深层网络中尤为重要，防止梯度消失。例如，在 6 层编码器中，残差连接确保深层信息不丢失。

###### 为什么要归一化？
- 归一化（Layer Normalization）确保每个层的输入分布稳定，避免数值不稳定性，提高训练效率。
- 例如，在训练过程中，层归一化帮助模型适应不同输入规模，加速收敛。

##### 3. 解码器（Decoder）
###### 解码器的详细过程
- **结构**：解码器也由 \(N = 6\) 个相同的层组成，每层包含三个子层：
  1. **掩码多头自注意力（Masked Multi-Head Self-Attention）**
  2. **编码器-解码器注意力（Encoder-Decoder Attention）**
  3. **位置-wise 前馈神经网络**
- **残差连接和层归一化**：与编码器类似，每个子层都有残差连接和层归一化。

###### 为什么 masked 后的结果作为 Q？
- 在解码器的自注意力中，查询（Q）来自解码器的当前层输出。
- 为了保持自回归性（即生成当前词时只能看到之前的词），需要对自注意力施加掩码，确保当前位置不能看到后续位置。
- 例如，在生成句子时，当前词“is”不能依赖后面的“blue”，掩码确保模型只看之前的词。

###### 为什么 encoder 的结果作为 K、V？
- 在编码器-解码器注意力中，解码器需要关注编码器的输出（即输入序列的表示）。
- 因此，编码器的输出作为键（K）和值（V），而解码器的输出作为查询（Q）。
- 这允许解码器在生成输出时参考整个输入序列。例如，在翻译中，解码器可直接利用编码器的上下文信息。

#### 多头注意力机制与归一化方法
##### 1. 传统注意力机制与自注意力机制
- **传统注意力机制**：
  - 在序列到序列模型中，解码器通过注意力机制关注输入序列的不同部分。
  - 例如，在机器翻译中，解码器生成法语词时，会关注英语输入序列中相关词。

- **自注意力机制**：
  - 查询（Q）、键（K）和值（V）都来自同一个序列。
  - “自”体现在 Q、K、V 均来自输入序列本身，而不是来自不同的序列。
  - 例如，在句子“猫在睡觉”中，自注意力允许“猫”直接与“睡觉”建立关系。

- **注意力机制的计算公式**：
  \[
  \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
  \]
  其中 Q、K、V 分别是查询、键和值矩阵，\( d_k \) 是键的维度。
- **举例说明**：
  - 假设序列为 "I love cats"，每个词的嵌入为 E1, E2, E3。
  - 对于位置 2（"love"），Q2 = Wq * E2，K = Wk * [E1; E2; E3]，V = Wv * [E1; E2; E3]。
  - 计算注意力分数：softmax( (Q2 K^T) / sqrt(d_k) )，然后与 V 相乘得到输出。

##### 2. 为什么用多头注意力机制？
- **多头比单头的好处**：
  - 多头注意力机制通过多个并行注意力头，从不同子空间提取信息，捕捉更丰富的特征。
  - 例如，一个头可能关注局部语法关系，另一个头关注全局语义。
- **详细计算过程**：
  - 假设 \(h = 8\) 个头，\(d_{\text{model}} = 512\)，则每个头的 \(d_k = d_v = 64\)。
  - 对于每个头 i，计算 Q_i = W_q^i * X，K_i = W_k^i * X，V_i = W_v^i * X。
  - 然后并行计算每个头的注意力：head_i = Attention(Q_i, K_i, V_i)。
  - 最后拼接所有头：MultiHead(Q, K, V) = Concat(head_1, ..., head_h) * W_o。
- **举例说明**：
  - 对于序列 "I love cats"，每个头可能关注不同的关系，如主语-动词或动词-宾语。

##### 3. LayerNorm 与 BatchNorm
- **LayerNorm（层归一化）**：
  - 计算公式：对于输入 x，计算均值 μ 和方差 σ^2 沿特征维度。
  - 归一化：\( \hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} \)。
  - 缩放和偏移：\( y = \gamma \hat{x} + \beta \)，其中 γ 和 β 是可学习的参数。
  - 适用场景：独立于批量大小，适合变长序列。

- **BatchNorm（批归一化）**：
  - 计算公式：沿批量维度计算均值和方差，归一化后缩放和偏移。
  - 适用场景：依赖于批量大小，适合固定大小的批量处理。

- **区别分析**：
  - LayerNorm 在每个样本内归一化，BatchNorm 在批量内归一化。
  - Transformer 选择 LayerNorm，因为序列长度可变，批归一化不适用。

- **为什么 Transformer 选择 LayerNorm**：
  - Transformer 处理变长序列，LayerNorm 不依赖批量大小，更稳定。
  - 文献如 [A survey of transformers](https://www.sciencedirect.com/science/article/pii/S2666651022000146) 指出，LayerNorm 帮助模型在深层网络中训练更高效。

#### 总结
Transformer 模型通过自注意力机制和前馈神经网络的组合，实现了高效的序列处理能力。它解决了 RNN 在并行化和长距离依赖方面的局限性，并在各种 NLP 任务中取得了突破性进展。嵌入、编码器和解码器的设计，以及多头注意力机制和层归一化的使用，使其成为现代深度学习中的核心架构。

- ##### 关键要点

  - Transformer 模型的核心是编码器和解码器，通过自注意力机制处理序列数据。
  - 相较于 RNN 和 CNN，Transformer 解决了并行计算和长距离依赖问题。
  - 嵌入部分包括词嵌入和位置编码，位置编码用公式计算。
  - 编码器和解码器各有多个层，包含多头自注意力机制和前馈网络。
  - 多头注意力机制允许多个并行关注点，层归一化稳定训练。
