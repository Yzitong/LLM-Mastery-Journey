# 2. 多头注意力机制与归一化方法

B站指路：[强烈推荐！台大李宏毅自注意力机制和Transformer详解！_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1v3411r78R/?spm_id_from=333.1387.favlist.content.click&vd_source=aa6afb9d0536d09ecdcb5d2c1fcf4c79)

### 引言

注意力机制的设计灵感源于人类对信息的处理方式 —— 我们在理解文本或图像时，会自然地关注重要部分，忽略次要信息。在深度学习中，它的核心作用是：

- **动态权重分配**：为输入序列中的每个元素（如句子中的单词、图像中的像素）计算一个 “注意力权重”，权重越高，代表该元素对当前任务的重要性越高。
- **上下文编码**：通过加权求和的方式，将关键信息整合到当前位置的表示中，形成包含上下文语义的编码。

### 1. 传统注意力机制与自注意力机制

- **传统注意力机制**（Attention）：
  - 在Transformer之前就有注意力机制，其含义是在序列到序列模型中，**解码器**通过注意力机制关注**输入序列**的不同部分。
  - 例如，在机器翻译中，解码器生成法语词时，会关注英语输入序列中相关词。

- **自注意力机制**（self-Attention）：
  - 查询（Q）、键（K）和值（V）都来自同一个序列。查询Q可以看做是一个问题，代表当前位置想 “找什么” ；K代表所有位置 “有什么” 信息，V是代表所有位置 “要融合的语义” 。通过注意力机制，来获取句子上下文的语义信息。
  - “self”主要体现在 Q、K、V **均来自输入序列本身**，而不是来自不同的序列。
  - 例如，在句子“猫在睡觉”中，自注意力允许“猫”直接与“睡觉”建立关系。
  - **注意力机制的计算公式**：
    ![image-20250630214113046](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250630214113046.png)

- **举例说明**：
  - 假设序列为 "我爱水课"，每个词的嵌入为 E1, E2, E3，E4。
  - 对于位置 2（"爱"），与其他3个字进行关联度的计算。
    - ![image-20250630220910787](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250630220910787.png)
    - ![image-20250630221002667](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250630221002667.png)
    - ![image-20250630221022139](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250630221022139.png)
  - 计算注意力分数：softmax( (Q2 K^T) / sqrt(d_k) )，然后与 V 相乘得到输出。
  - ![image-20250630221700836](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250630221700836.png)
- **为什么要除以根号dk呢？**

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250630223006226.png" alt="image-20250630223006226" style="zoom: 67%;" />

### 2. 为什么用多头注意力机制？

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250630223146794.png" alt="image-20250630223146794" style="zoom: 50%;" />

- **多头比单头的好处**：
  
  - 多头注意力机制通过多个并行注意力头，从不同子空间提取信息，捕捉更丰富的特征。
  - 例如，一个头可能关注局部语法关系，另一个头关注全局语义。
  
- **详细计算过程**：
  
  - 假设 \(h = 8\) 个头，\(d_{\text{model}} = 512\)，则每个头的 \(d_k = d_v = 64\)。
  - 对于每个头 i，计算 Q_i = W_q^i * X，K_i = W_k^i * X，V_i = W_v^i * X。
  - 然后并行计算每个头的注意力：head_i = Attention(Q_i, K_i, V_i)。
  - 最后拼接所有头：MultiHead(Q, K, V) = Concat(head_1, ..., head_h) * W_o。
  
- **举例说明**：
  
  - 对于序列 "I love cats"，每个头可能关注不同的关系，如主语-动词或动词-宾语。
  
- self - Attention的代码

  

### 3. 归一化的方法

- **LayerNorm（层归一化）**：
  - 计算公式：对于输入 x，计算均值 μ 和方差 σ^2 沿特征维度。
  - 归一化：\( \hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} \)。
  - 缩放和偏移：\( y = \gamma \hat{x} + \beta \)，其中 γ 和 β 是可学习的参数。
  - 适用场景：独立于批量大小，适合变长序列。

- **BatchNorm（批归一化）**：
  - 计算公式：沿批量维度计算均值和方差，归一化后缩放和偏移。
  - 适用场景：依赖于批量大小，适合固定大小的批量处理。

- **区别分析**：
  - LayerNorm 在每个样本内归一化，BatchNorm 在批量内归一化。
  - Transformer 选择 LayerNorm，因为序列长度可变，批归一化不适用。

- **为什么 Transformer 选择 LayerNorm**：
  - Transformer 处理变长序列，LayerNorm 不依赖批量大小，更稳定。
  - 序列长度可变，LayerNorm 更稳定，适合深层网络。

